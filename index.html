<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="description"
          content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
    <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>VisTacPoint</title>
  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
  
      function gtag() {
        dataLayer.push(arguments);
      }
  
      gtag('js', new Date());
  
      gtag('config', 'G-PYVRSFMDRL');
    </script>
  
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">
  
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
  
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <style>
      body {
          margin: 0;
          padding: 10px; /* 页面两边的间距 */
          font-family: Arial, sans-serif;
      }

      section {
          margin-bottom: 10px; /* 每个section之间的间距 */
      }

      /* Section 1 样式 */
      .section1 {
          background-color: #f0f8ff;
          padding: 5px;
          /* border-left: 5px solid #007BFF; */
          font-size: 20px; /* 字体大小 */
          font-weight: bold; /* 字体粗体 */
      }

      /* Section 2 样式 */
      .section2 {
          /* background-color: #f8d7da; */
          padding: 10px;
          /* border-left: 5px solid #dc3545; */
          font-size: 8px; /* 字体大小 */
          font-weight: bold; /* 字体粗体 */
          color: #d00606;
          margin-top: 3px;
      }

      /* Section 3 样式 */
      .section3 {
          /* background-color: #f8d7da; */
          padding: 1px;
          /* border-left: 5px solid #dc3545; */
          font-size: 8px; /* 字体大小 */
          font-weight: bold; /* 字体粗体 */
          color: #220202;
      }
      .sectionab {
      /* 你可以根据需要调整section的外边距 */
      margin: 7px;
    }
 
    .publication-abs {
      text-align: justify; /* 设置文本两端对齐 */
      font-size: 20px; /* 设置字体大小 */
      padding: 0 10px; 
      font-weight: bold
      /* 如果需要上下内边距，可以添加类似下面的行 */
      /* margin-top: 10px; */
      /* margin-bottom: 10px; */
    }

 
    .publication-text {
      text-align: justify; /* 设置文本两端对齐 */
      font-size: 20px; /* 设置字体大小 */
      padding: 1px 40px; /* 增加上下和左右的内边距，左右设置为40px以增加两端距离 */
      font-weight: bold; /* 字体加粗 */
      /* 如果需要上下外边距，可以添加类似下面的行 */
      /* margin-top: 10px; */
      /* margin-bottom: 10px; */
      line-height: 1.6; /* 可选：增加行高以提高可读性 */
    }
/* 
      .img {
              margin-top: 1px; 
            }
          
            .img img {
              max-width: 60%; 
              height: auto;
            }
          
            .img .content {
              margin-top: 3px; 
              font-size: 5px;
              text-align: center;
              line-height: 1.6; 
            } */
    .img img {
        max-width: 50%;
        /* width: 1200px;  */
        height: auto; /* 保持图片宽高比 */
      }
 
  /* 设置当前 section 与上一个 section 的间距 */
      .img {
        margin-top: 5px; /* 调整间距大小，例如 20px */
      }
  </style>
  </head>
<body>
  <section class="section1">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- 标题 -->
            <h1 class="title is-1 publication-title">VisTacPoint: A Visual and Tactile Multi-modal Fusion Dataset towards Embodied Perception</h1>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="section2">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Feng Luan, </span>
              <span class="author-block">Jiarui Hu, </span>
              <span class="author-block">Changshi Zhou, </span>
              <span class="author-block">Zheng Yan, </span>
              <span class="author-block">Zhipeng Wang, </span>
              <span class="author-block">Yanmin Zhou, </span>
              <span class="author-block">Jiguang Yue, </span>
              <span class="author-block">Bin He</span>
            </div>
   
            <div class="is-size-5 publication-authors">
              <span class="author-block">Tongji University</span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="sectionab">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-abs">
              Abstract: Collecting and understanding three-dimensional (3D) information is critical to improving the intelligence of robots. 
              Robots typically rely on vision devices equipped at fixed positions to acquire 3D information about target objects. 
              However, the data captured by the robot is often limited to a single perspective, resulting in partial information and posing significant 
              challenges to subsequent tasks such as grasping and navigation.
              Tactile sense, an essential part of robotic perception, gives robots more precise local object features. 
              The fusion of vision and tactile is appealing, which can help robots to fully understand object properties and shapes to improve the success 
              rate of robotic task execution. Unfortunately, there is a lack of comprehensive real-world datasets that integrate information from these two 
              perceptual modalities. Therefore, we have meticulously constructed a spatially aligned visual-tactile dataset derived from real-world scenarios, 
              called VisTacPoint, which includes visual and tactile point clouds as well as raw tactile data. The VisTacPoint dataset contains data for a total 
              of 207 desktop objects across 8 categories. Our dataset can be widely applied to object understanding tasks such as 3D reconstruction, 
              object recognition based on vision-tactile fusion, empowering robotic embodied and spatial intelligence. Furthermore, to accommodate diverse task 
              requirements, we have constructed multiple variants of VisTacPoint for selection. Multiple visual-tactile fusion methods were applied to VisTacPoint 
              for different tasks. Finally, the challenges of multi-modal fusion for robotic embodied perception are summarized.
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="img">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/FIGBRAIN.png" alt="Centered Image"/>  
    </div>
  </section>
  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 1: Multi-modal learning process and its applications. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="sectionfig1text">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              Multi-modal learning is a key area in Artificial Intelligence (AI) research, focusing on integrating information from diverse sources, 
              such as vision, sound, text and touch, to enhance machine learning models [18][19][20]. Recently, this field has witnessed significant 
              advancements and broad applications. This paper specifically explores fusion techniques for visual and tactile data. 
              Visual-tactile fusion is crucial for enabling robotic systems to achieve comprehensive object understanding and dexterous manipulation, 
              mirroring human-like interaction capabilities. Visual sensors can capture high-resolution images, enabling robots to accurately identify 
              the position and shape of objects [21][22]. Tactile sensors, on the other hand, can detect physical properties such as hardness and 
              texture [23][24]. Combining these two types of data can improve recognition accuracy, particularly for objects that appear similar but 
              differ in physical characteristics. The visual-tactile multi-modal learning process and its applications are illustrated in Fig. 1. The 
              multi-modal perception model acts as the brain of a robot, providing the cognitive foundation for subsequent tasks, such as object 
              understanding and robotic grasping, by processing data from its sensing devices.
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <section class="img">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/table.png" alt="Centered Image"/>  
    </div>
  </section>

  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Table 1 A summary of datasets available for visual-tactile fusion. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="img">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/device.png" alt="Centered Image"/>  
    </div>

    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              High-quality datasets are essential for training and improving multi-modal learning models [33][34]. Datasets can provide sufficient 
              training samples for the model to adequately learn the desired features and ensure that the model operates effectively in different 
              scenarios and conditions [35]. By leveraging well-designed and accurately labeled multi-modal datasets, models can better understand 
              the relationships and complementary features between different modalities, enabling deeper integration and more efficient utilization of 
              complex information.
   

            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              In recent years, numerous visual-tactile multi-modal datasets for object understanding or robotic manipulation have emerged, 
              including Touch100k [16] and OBJECTFOLDER 1.0 [36], which we have concisely summarized in Table 1. Table 1 outlines several key 
              characteristics of these datasets, such as their names, data types, object quantities and sources. OBJECTFOLDER 1.0 [36] introduced 
              a dataset comprising 100 virtualized objects, each characterized by visual textures, acoustic simulations and tactile feedback, all 
              encapsulated within a unified, object-centered implicit representation. OBJECTFOLDER 2.0 [37] extends OBJECTFOLDER 1.0 even further, 
              increasing the number of virtual objects from 100 to 1,000, with greatly improved multi-modal rendering fidelity for visual, auditory 
              and tactile modalities. To further investigate robotic manipulation, Wang et al. [38] constructed a multi-modal grasping dataset based 
              on dexterous robotic hands. The dataset contains 2,550 data samples, including tactile data, joint information, timestamps, images and 
              RGB-D videos. By integrating visual and tactile data, robots can perform manipulation tasks more efficiently. Additionally, VisGel [39]
              and VinT-6D [42] can be used for robotic grasping tasks. The datasets Touch and Go [40], Touch100k [16], and TVL [41] are based on 
              vision cameras and tactile sensors (GelSight) for acquiring robotic multi-modal data, which can be applied to understanding object 
              surface properties.
        
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">

              Point clouds can explicitly represent 3D structures through a dense set of points, enabling accurate geometric reconstruction and 
              depth perception, with significant advantages in spatial understanding [45][46]. The integration of visual and tactile modalities into 
              a unified point cloud framework (called a visual-tactile point cloud) allows us to exploit the complementary strengths of both modalities. 
              Visual-tactile point cloud or voxel (point cloud voxelization) datasets, such as those in Rustler et al. [43] and David et al. [44], 
              are currently oriented towards object reconstruction tasks. David et al. [44] constructed a synthetic visual-tactile voxel dataset 
              containing 608 objects. Rustler et al. [43] constructed a visual-tactile point cloud dataset containing 87 unique grids from the YCB 
              database [47] and the Grasp database [48]. However, the tactile point clouds/voxels in these datasets are randomly selected by following 
              the true values of the occluded parts, rather than obtained from actuality. In addition, these datasets lack further processing, such as 
              object classification or part segmentation, resulting in a lack of further use in more applications. In this paper, our aim is to construct 
              a visual-tactile point cloud dataset from the real world for object understanding.

            </div>
          </div>
        </div>
      </div>
    </div>

  </section>
  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 2:  Multi-modal sensing platform and scanning device in our experiments. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



  <style>
    /* 确保所有文字居中 */
    .has-text-centered {
      text-align: center;
    }
   
    /* 去掉作者名字的超链接样式，仅保留文本 */
    .publication-authors .author-block a {
      text-decoration: none; /* 去掉下划线 */
      color: inherit;         /* 继承父元素颜色 */
      pointer-events: none;   /* 禁用鼠标事件，防止点击 */
      cursor: default;        /* 鼠标指针变为默认样式 */
    }
   
    /* 可选：优化链接按钮的显示，使整体看起来更整齐 */
    .publication-links .link-block {
      margin: 0 5px;
    }
  </style>
</body>
</html>
