<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="description"
          content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
    <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>VisTacPoint</title>
  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
  
      function gtag() {
        dataLayer.push(arguments);
      }
  
      gtag('js', new Date());
  
      gtag('config', 'G-PYVRSFMDRL');
    </script>
  
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">
  
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
  
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <style>
      body {
          margin: 0;
          padding: 10px; /* 页面两边的间距 */
          font-family: Arial, sans-serif;
      }

      section {
          margin-bottom: 30px; /* 每个section之间的间距 */
      }

      /* Section 1 样式 */
      .section1 {
          background-color: #f0f8ff;
          padding: 10px;
          /* border-left: 5px solid #007BFF; */
          font-size: 15px; /* 字体大小 */
          font-weight: bold; /* 字体粗体 */
      }

      /* Section 2 样式 */
      .section2 {
          /* background-color: #f8d7da; */
          padding: 10px;
          /* border-left: 5px solid #dc3545; */
          font-size: 8px; /* 字体大小 */
          font-weight: bold; /* 字体粗体 */
          color: #d00606;
      }

      /* Section 3 样式 */
      .section3 {
          /* background-color: #f8d7da; */
          padding: 10px;
          /* border-left: 5px solid #dc3545; */
          font-size: 8px; /* 字体大小 */
          font-weight: bold; /* 字体粗体 */
          color: #220202;
      }


      .img {
              margin-top: 5px; /* 减少与上一个 section 的距离 */
            }
          
            .img img {
              max-width: 60%; /* 设置图片宽度为容器宽度的 80%，可以根据需要调整 */
              height: auto; /* 保持图片宽高比 */
            }
          
            .img .content {
              margin-top: 3px; /* 图片与描述文字的间距 */
              font-size: 5px;
              text-align: center;
              line-height: 1.6; /* 增加文字的可读性 */
            }

  </style>
  </head>
<body>
  <section class="section1">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- 标题 -->
            <h1 class="title is-1 publication-title">VisTacPoint: A Visual and Tactile Multi-modal Fusion Dataset towards Embodied Perception</h1>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="section2">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Feng Luan, </span>
              <span class="author-block">Jiarui Hu, </span>
              <span class="author-block">Changshi Zhou, </span>
              <span class="author-block">Zheng Yan, </span>
              <span class="author-block">Zhipeng Wang, </span>
              <span class="author-block">Yanmin Zhou, </span>
              <span class="author-block">Jiguang Yue, </span>
              <span class="author-block">Bin He</span>
            </div>
   
            <div class="is-size-5 publication-authors">
              <span class="author-block">Tongji University</span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
 
  <section class="img">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/FIGBRAIN.png" alt="Centered Image"/>  
    </div>
  </section>
  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 1: Multi-modal learning process and its applications. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="abj" style="margin: 10px;">
    <div class="content has-text-justified"  style="text-align: justify; font-size: 20px;">
      <p>
        Abstract: Collecting and understanding three-dimensional (3D) information is critical to improving the intelligence of robots. 
        Robots typically rely on vision devices equipped at fixed positions to acquire 3D information about target objects. 
        However, the data captured by the robot is often limited to a single perspective, resulting in partial information and posing significant 
        challenges to subsequent tasks such as grasping and navigation.
        Tactile sense, an essential part of robotic perception, gives robots more precise local object features. 
        The fusion of vision and tactile is appealing, which can help robots to fully understand object properties and shapes to improve the success 
        rate of robotic task execution. Unfortunately, there is a lack of comprehensive real-world datasets that integrate information from these two 
        perceptual modalities. Therefore, we have meticulously constructed a spatially aligned visual-tactile dataset derived from real-world scenarios, 
        called VisTacPoint, which includes visual and tactile point clouds as well as raw tactile data. The VisTacPoint dataset contains data for a total 
        of 207 desktop objects across 8 categories. Our dataset can be widely applied to object understanding tasks such as 3D reconstruction, 
        object recognition based on vision-tactile fusion, empowering robotic embodied and spatial intelligence. Furthermore, to accommodate diverse task 
        requirements, we have constructed multiple variants of VisTacPoint for selection. Multiple visual-tactile fusion methods were applied to VisTacPoint 
        for different tasks. Finally, the challenges of multi-modal fusion for robotic embodied perception are summarized.

      </p>
    </div>
  </section>
  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Table 1 A summary of datasets available for visual-tactile fusion. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="img">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/table.png" alt="Centered Image"/>  
    </div>
  </section>


  <section class="img">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/device.png" alt="Centered Image"/>  
    </div>
  </section>
  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 2:  Multi-modal sensing platform and scanning device in our experiments. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



  <style>
    /* 确保所有文字居中 */
    .has-text-centered {
      text-align: center;
    }
   
    /* 去掉作者名字的超链接样式，仅保留文本 */
    .publication-authors .author-block a {
      text-decoration: none; /* 去掉下划线 */
      color: inherit;         /* 继承父元素颜色 */
      pointer-events: none;   /* 禁用鼠标事件，防止点击 */
      cursor: default;        /* 鼠标指针变为默认样式 */
    }
   
    /* 可选：优化链接按钮的显示，使整体看起来更整齐 */
    .publication-links .link-block {
      margin: 0 5px;
    }
  </style>
</body>
</html>
