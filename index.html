<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="description"
          content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
    <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>VisTacPoint</title>
  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
  
      function gtag() {
        dataLayer.push(arguments);
      }
  
      gtag('js', new Date());
  
      gtag('config', 'G-PYVRSFMDRL');
    </script>
  
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">
  
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
  
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <style>
      body {
          margin: 0;
          padding: 10px; /* 页面两边的间距 */
          font-family: Arial, sans-serif;
      }

      section {
          margin-bottom: 10px; /* 每个section之间的间距 */
      }

      /* Section 1 样式 */
      .section1 {
          background-color: #f0f8ff;
          padding: 5px;
          /* border-left: 5px solid #007BFF; */
          font-size: 20px; /* 字体大小 */
          font-weight: bold; /* 字体粗体 */
      }

      /* Section 2 样式 */
      .section2 {
          /* background-color: #f8d7da; */
          padding: 10px;
          /* border-left: 5px solid #dc3545; */
          font-size: 8px; /* 字体大小 */
          font-weight: bold; /* 字体粗体 */
          color: #d00606;
          margin-top: 3px;
      }

      /* Section 3 样式 */
      .section3 {
          /* background-color: #f8d7da; */
          padding: 1px;
          /* border-left: 5px solid #dc3545; */
          font-size: 8px; /* 字体大小 */
          font-weight: bold; /* 字体粗体 */
          color: #220202;
      }
      .sectionab {
      /* 你可以根据需要调整section的外边距 */
      margin: 7px;
    }
 
    .publication-abs {
      text-align: justify; /* 设置文本两端对齐 */
      font-size: 20px; /* 设置字体大小 */
      padding: 0 10px; 
      font-weight: bold
      /* 如果需要上下内边距，可以添加类似下面的行 */
      /* margin-top: 10px; */
      /* margin-bottom: 10px; */
    }

 
    .publication-text {
      text-align: justify; /* 设置文本两端对齐 */
      font-size: 20px; /* 设置字体大小 */
      padding: 1px 40px; /* 增加上下和左右的内边距，左右设置为40px以增加两端距离 */
      font-weight: bold; /* 字体加粗 */
      /* 如果需要上下外边距，可以添加类似下面的行 */
      /* margin-top: 10px; */
      /* margin-bottom: 10px; */
      line-height: 1.6; /* 可选：增加行高以提高可读性 */
    }
/* 
      .img {
              margin-top: 1px; 
            }
          
            .img img {
              max-width: 60%; 
              height: auto;
            }
          
            .img .content {
              margin-top: 3px; 
              font-size: 5px;
              text-align: center;
              line-height: 1.6; 
            } */
    .img img {
        max-width: 50%;
        /* width: 1200px;  */
        height: auto; /* 保持图片宽高比 */
      }
 
  /* 设置当前 section 与上一个 section 的间距 */
      .img {
        margin-top: 5px; /* 调整间距大小，例如 20px */
      }
    .image-container {
      display: flex;
      justify-content: center;
      align-items: center;
      margin-bottom: 20px; /* 可选：增加图片之间的间距 */
    }
  
    .image-small img {
      max-width: 20%;
      height: auto;
    }
  </style>
  </head>
<body>
  <section class="section1">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- 标题 -->
            <h1 class="title is-1 publication-title">VisTacPoint: A Visual and Tactile Multi-modal Fusion Dataset towards Embodied Perception</h1>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="section2">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Feng Luan, </span>
              <span class="author-block">Jiarui Hu, </span>
              <span class="author-block">Changshi Zhou, </span>
              <span class="author-block">Zheng Yan, </span>
              <span class="author-block">Zhipeng Wang, </span>
              <span class="author-block">Yanmin Zhou, </span>
              <span class="author-block">Jiguang Yue, </span>
              <span class="author-block">Bin He</span>
            </div>
   
            <div class="is-size-5 publication-authors">
              <span class="author-block">Tongji University</span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="sectionab">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-abs">
              Abstract: Collecting and understanding three-dimensional (3D) information is critical to improving the intelligence of robots. 
              Robots typically rely on vision devices equipped at fixed positions to acquire 3D information about target objects. 
              However, the data captured by the robot is often limited to a single perspective, resulting in partial information and posing significant 
              challenges to subsequent tasks such as grasping and navigation.
              Tactile sense, an essential part of robotic perception, gives robots more precise local object features. 
              The fusion of vision and tactile is appealing, which can help robots to fully understand object properties and shapes to improve the success 
              rate of robotic task execution. Unfortunately, there is a lack of comprehensive real-world datasets that integrate information from these two 
              perceptual modalities. Therefore, we have meticulously constructed a spatially aligned visual-tactile dataset derived from real-world scenarios, 
              called VisTacPoint, which includes visual and tactile point clouds as well as raw tactile data. The VisTacPoint dataset contains data for a total 
              of 207 desktop objects across 8 categories. Our dataset can be widely applied to object understanding tasks such as 3D reconstruction, 
              object recognition based on vision-tactile fusion, empowering robotic embodied and spatial intelligence. Furthermore, to accommodate diverse task 
              requirements, we have constructed multiple variants of VisTacPoint for selection. Multiple visual-tactile fusion methods were applied to VisTacPoint 
              for different tasks. Finally, the challenges of multi-modal fusion for robotic embodied perception are summarized.
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="img">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/FIGBRAIN.png" alt="Centered Image"/>  
    </div>
  </section>
  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 1: Multi-modal learning process and its applications. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              Multi-modal learning is a key area in Artificial Intelligence (AI) research, focusing on integrating information from diverse sources, 
              such as vision, sound, text and touch, to enhance machine learning models [18][19][20]. Recently, this field has witnessed significant 
              advancements and broad applications. This paper specifically explores fusion techniques for visual and tactile data. 
              Visual-tactile fusion is crucial for enabling robotic systems to achieve comprehensive object understanding and dexterous manipulation, 
              mirroring human-like interaction capabilities. Visual sensors can capture high-resolution images, enabling robots to accurately identify 
              the position and shape of objects [21][22]. Tactile sensors, on the other hand, can detect physical properties such as hardness and 
              texture [23][24]. Combining these two types of data can improve recognition accuracy, particularly for objects that appear similar but 
              differ in physical characteristics. The visual-tactile multi-modal learning process and its applications are illustrated in Fig. 1. The 
              multi-modal perception model acts as the brain of a robot, providing the cognitive foundation for subsequent tasks, such as object 
              understanding and robotic grasping, by processing data from its sensing devices.
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="sectionfig1text">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              Multi-modal learning is a key area in Artificial Intelligence (AI) research, focusing on integrating information from diverse sources, 
              such as vision, sound, text and touch, to enhance machine learning models [18][19][20]. Recently, this field has witnessed significant 
              advancements and broad applications. This paper specifically explores fusion techniques for visual and tactile data. 
              Visual-tactile fusion is crucial for enabling robotic systems to achieve comprehensive object understanding and dexterous manipulation, 
              mirroring human-like interaction capabilities. Visual sensors can capture high-resolution images, enabling robots to accurately identify 
              the position and shape of objects [21][22]. Tactile sensors, on the other hand, can detect physical properties such as hardness and 
              texture [23][24]. Combining these two types of data can improve recognition accuracy, particularly for objects that appear similar but 
              differ in physical characteristics. The visual-tactile multi-modal learning process and its applications are illustrated in Fig. 1. The 
              multi-modal perception model acts as the brain of a robot, providing the cognitive foundation for subsequent tasks, such as object 
              understanding and robotic grasping, by processing data from its sensing devices.
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  
  <section class="img">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/table.png" alt="Centered Image"/>  
    </div>
  </section>

  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Table 1 A summary of datasets available for visual-tactile fusion. </span>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              High-quality datasets are essential for training and improving multi-modal learning models. Datasets can provide sufficient 
              training samples for the model to adequately learn the desired features and ensure that the model operates effectively in different 
              scenarios and conditions. In recent years, numerous visual-tactile multi-modal datasets for object understanding or robotic manipulation have emerged, 
              including Touch100k and OBJECTFOLDER 1.0 , which we have concisely summarized in Table 1. Point clouds can explicitly represent 3D structures through a dense set of points, enabling accurate geometric reconstruction and 
              depth perception, with significant advantages in spatial understanding. The integration of visual and tactile modalities into 
              a unified point cloud framework (called a visual-tactile point cloud) allows us to exploit the complementary strengths of both modalities. 
              In this work, our aim is to construct a visual-tactile point cloud dataset from the real world for object understanding.
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="img">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/device.png" alt="Centered Image"/>  
    </div>

   

  </section>
  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 2:  Multi-modal sensing platform and scanning device in our experiments. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              This experimental platform integrates a vision camera and a robotic arm equipped with a tactile sensor, as shown in Fig. 2. 
              The Intel RealSense D455 camera is used as the vision sensor, providing high-resolution RGB images and depth maps to support 
              complex scene understanding and object recognition. The UR5 robotic arm is employed as the robotic manipulator, enabling precise 
              operations through its six-degree-of-freedom design, high load capacity and flexible motion control. The self-developed E-Skin sensor
               serves as the tactile sensor.
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="experiencevedio">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-15 publication-text">
              Calibration process for spatial position recognition of tactile sensors:
            </div>
          </div>
        </div>
      </div>
    </div>
    <div style="display: flex; justify-content: center;">
      <video autoplay controls muted loop playsinline style="width: 25%; justify-content: center;">
        <source src="./static/images/tactile_cali.mp4" type="video/mp4">
      </video>
    </div>
  </section>
  <section class="section3">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/process.png" alt="Centered Image" style="width: 80px; height: auto;"/>  
    </div>
    <!-- <div class="image-container image-small">
      <img src="./static/images/process.png" alt="Small Image"/>
    </div> -->
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 3:  The data collection process in our experiments. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/tactile.png" alt="Centered Image"/>  
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 4:  The tactile data collection process. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              The entire tactile data collection process is illustrated in Fig. 3. Additionally, we demonstrate the real-world scenario of a 
              robotic arm equipped with tactile sensors touching an object to acquire data, as shown in Fig. 4. The tactile data collection 
              process involves controlling the UR5 to touch with objects, relying on our independently developed E-Skin sensor. 
              The core idea of this process is to perceive locations that are not directly observable through vision, especially those 
              parts with distinctive features. This core idea for exploration is consistent with the requirements of vision-tactile fusion scenarios. 
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section3">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/practiveob.png" alt="Centered Image"/>  
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 5:  Partial objects in the dataset. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/Inputshow.png" alt="Centered Image"/>  
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 6: Visual-tactile point clouds and ground truth. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/truth.png" alt="Centered Image"/>  
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 7: Part segmentation results and point cloud distortion in point cloud acquired by camera. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/dadaanalysis.png" alt="Centered Image"/>  
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 8: Percentage of each category in VisTacPoint. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              
              the visual point cloud, tactile point cloud, tactile data and ground truth are paired to form our 
              VisTacPoint dataset. Fig. 5 shows some objects included in VisTacPoint. Fig. 6 displays the ground 
              truth of two objects with part labels, where the white points in the red box represent distortions caused 
              by the real camera. Fig. 7 illustrates the visual-tactile point clouds and the ground truth in VisTacPoint.
              As shown in Figure 8, we summarise the categories of all objects and their percentages.
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="section3">
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/filelist.png" alt="Centered Image"/>  
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 9:  Summary of VisTacPoint dataset documentation.. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
    
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              VisTacPoint contains a total of 207 objects divided into eight categories. It is available in two versions: 
              one is a pure vision version, which includes pairs of single-view point clouds and complete point clouds; 
              the other is a visual-tactile version, encompassing single-view point clouds, tactile point clouds, tactile data 
              and ground truth pairs. We summarized the collected and processed data files as shown in Fig. 9. 
              
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              Furthermore, to simulate scenarios from different viewpoints and poses, we rotated the data five times 
              by 60 degrees along each of the x, y, and z axis, thereby increasing the size of the dataset. After data 
              selection and processing, datasets in the pure vision version (VisTacPoint-Vl) and the visual-tactile version (VisTacPoint-V2) 
              was determined.
              
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              VisTacPoint datasets can be used for point cloud analysis tasks such as classification, reconstruction and part segmentation.
              The classification and reconstruction results are shown in Figs. 10 and 11.
              
            </div>
          </div>
        </div>
      </div>
    </div>
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/classifica.png" alt="Centered Image"/>  
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 10:  Classification results. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div style="display: flex; justify-content: center; align-items: center;">  
      <img src="./static/images/reconstruction.png" alt="Centered Image"/>  
    </div>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fig. 11:  Qualitative analyses for reconstruction results. </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section3">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-30 publication-text">
              Contributions:
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-8 publication-text">
              The contributions of the VisTacPoint dataset are multi-faceted: it is derived from real-world sources, 
              facilitates the development of more robust and generalizable models that are easily deployable in real-world scenarios, 
              and sets a new benchmark for future research in point cloud analysis. By focusing on single-view point clouds and supplementing
              them with tactile data to address occlusion problems, the dataset is closely aligned with real-world applications and provides a 
              valuable resource for developing and evaluating algorithms under constrained sensing conditions.
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  
  <style>
    /* 确保所有文字居中 */
    .has-text-centered {
      text-align: center;
    }
   
    /* 去掉作者名字的超链接样式，仅保留文本 */
    .publication-authors .author-block a {
      text-decoration: none; /* 去掉下划线 */
      color: inherit;         /* 继承父元素颜色 */
      pointer-events: none;   /* 禁用鼠标事件，防止点击 */
      cursor: default;        /* 鼠标指针变为默认样式 */
    }
   
    /* 可选：优化链接按钮的显示，使整体看起来更整齐 */
    .publication-links .link-block {
      margin: 0 5px;
    }
  </style>
</body>
</html>
